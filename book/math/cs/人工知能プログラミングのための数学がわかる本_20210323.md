# 本「人工知能プログラミングのための数学がわかる本」
- 2021年3月23日より

## 1. 数学基礎
- ラジアンの定義
    - 半径rの円の弧長rに対応する偏角を1
- n次元のユークリッド距離の定義がない

## 2. 微分
- \frac{dx(t)}{dt} ではなく \frac{dx}{dt}(t) の方が自然な気がした
    - \frac{dx}{dt}で導関数という１つの関数
    - \frac{d}{dt}x(t)は演算子と変数
    - 3冊の数学書を確認したがそもそも下付き文字を使うことが多い
- 微分演算子を分数のように扱っていて筆者に不信感
    - p.59
- ReLu関数をmax(0,x)と表すの、AtCoderっぽい
    - 実装で使いやすそう
- ReLU関数がx=0で微分不可能なことに明記がない
    - NNで使う上で便宜的にφ’(0)=0とするらしい
        - https://www.atmarkit.co.jp/ait/articles/2003/11/news016.html

## 3. 線形代数
- ベクトルの導入が曖昧
    - 一次元ベクトル \sim スカラー が伝わらない
- ベクトル表記の用途が嘘くさい
    - \bm は一般によく用いられる？
        - 成分としての数値ベクトルに利用 では
    - \vec は高校で使われる？
        - 物理での空間ベクトルに利用 では
- fasttext
    - Word2Vecの拡張
        - 単語->ベクトルから
        - 言語->ベクトルへ
    - Facebook (2016)
- 行列の積は内積の拡張
- 線形変換
    - AxのAの列成分が基底ベクトルの射影先
    - 可視化がわかりやすいサイト
        - 線形変換とは？誰でも必ず理解できるようにアニメーションで解説 https://www.headboost.jp/what-is-linear-transformation/ 
- 「（2次正方行列の）固有ベクトルと固有値のペアは必ず2つある」は誤り。
    - 回転行列や剪断行列など、ペアが0個や1個の場合も存在する
    - https://www.headboost.jp/matrix-eigenvalues/

## 4. 確率統計
- 結合確率
    - P(A \cap B) = P(A,B)
- 陽性検査
    - 発症率が低いと99.99%の精度でも正しく判定できるのは5%ほど
    - 再検査で99.8%に
- 平均と期待値が等価？
    - ほんとう？
- 共分散
    - データの相関
        - 強弱はまだいえない
            - 数字の大きさ依存するため
            - 単位が異なるため
    - 単位の異なるもの同士でも可能
- 偏差値
    - 平均60点のときの80点と平均30点はどちらが優れているか
- ベイズ推定
    - 最尤推定に近い
    - 怪しいが実用的

## 5. 実践編１
- 正則化
    - なぜ重み関数を追加するとモデルが複雑にならなくなるのか記載がない
- モデルの評価
    - 残差を表示する方法
        - 偏りがないか
        - 一様に分布しているか
            - 直線などがないか

## 6. 実践編２
- n-gram
    - n字ずつ反転して辞書がなくても解析可能に
- ストップワード
    - 特徴に関係ない助詞などを排除
- BoW
    - 単語の頻度をベクトル化
- アダマール積
    - 行列の各要素を積算
    - より重要な単語に重みづけ
        - 自動にできない？
- TF-IDF
    - TFとIDFの積
    - 重みづけの手法
    - TF=単語数/全単語数
    - -log_{n(>1)}文数/全文数
        - logは変化によく反応させるため
            - logは0に近い程急速に無限に近づくから？
- ロジスティック回帰
    - 分類が曖昧なものを判別することが得意
    - 類似度を確率で表現（～～寄り）
    - ベルヌーイ分布から0 or 1となる確率を導出
    - ロジスティック関数
        - p(y|x;Θ)=1/(a+exp(-Θ^Tx))
    - コスト関数
        - クロスエントロピー誤差関数
        - 3分類以上は1つの分類か否かの2値分類問題へ

## 7. 実践編３
- DNNが人間より優れた性能 は嘘
- MNIST
    - 255で割って正規化した方が速い
        - 都度255で割るのではなく、はじめに割っておくから？
- 順伝播
    - x^1 = W^1 a^0 + b ^ 1
    - 活性化関数でDNNの表現が上がるのはなぜ？
- softmax関数
    - 出力層への非活性化関数？
    - y_i = exp(x_i) = \sum_k exp(x_k)
    - [0,1]の確率
        - \sum_i y_i = 1
    - なぜexpに通す？
    - どんなグラフになる？
        - 特殊なシグモイドっぽい形
        - 上下反転、曲率さまざま
        - https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.atmarkit.co.jp%2Fait%2Farticles%2F2004%2F08%2Fnews016.html&psig=AOvVaw0yf9F_NobEL13OLv7oenv4&ust=1617414787203000&source=images&cd=vfe&ved=0CA0QjhxqFwoTCOCU9OO53u8CFQAAAAAdAAAAABAD
    - 層の数やノード数は半手動
- DNNの損失関数
    - クロスエントロピーが多い
        - -\sum t ln y
        - 0（出力誤り）ほど急速に大きくなり、問題視される
        - y=1に吸い込まれる
- one-hot表現
    - 正解だけ1、他は0
- 勾配降下法
    - E = - \ita (\nabura_w E^T) ( |\ita| < 1 ) のとき降下
    - 応用
        - 確率的勾配降下法
            - 収束速度を上げるため、計算する重みを確率で選択
        - バッチサイズ
            - 選択する数
        - エポック数
            - 使いまわしの数
        - ex バッチサイズ2000, エポック数50, 60000/2000 = 30回/エポック
- 誤差逆伝播法
    - 大量の微分演算の回避
    - 2乗誤差と標準シグモイド関数
    - チェーンルールで求める勾配を前の勾配で表す
        - 偏E/x_new -> 前の\delta
        - 偏x_new/a -> 単項のa
        - 偏a/x_old -> シグモイド入力後の微分
    - 結論
        - 最終層
            - \delta = (a - y) \zeta'
        - それ以前
            - \delta = \sum (\delta w) \zeta'
        - バイアス
            - a^{l-1} = 1 より \delta = 偏E/b
- DNN
    - バッチサイズの数だけ順伝搬
    - 勾配降下法・誤差逆伝播の繰り返し
- ドロップアウト法
    - 汎化させる正則化の代わり
    - randomでニューロンを消す
    - ドロップアウトエイルは20%ほど？
